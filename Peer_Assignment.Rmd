---
title: "Peer Assignment Practical Machine Learning"
output: 
  html_document: 
    keep_md: yes
---
                                        
                                        
                                           K. van Splunter
                  
                  
                                        
## Introduction

This project focuses on machine learning. This document will build a prediction-model and test this model.  
The data that is being used in this project has data of 6 participants about their performance of barbell lifts. They were asked to do it correctly and incorrectly in 4 different ways.
The data comes from the following source: http://groupware.les.inf.puc-rio.br/har

## Data

First, the data is loaded into R and the necessary packages are loaded. The seed is set to ensure that it's reproducible.
```{r loadDat}
trainingData <- read.csv("Trainingdata.csv", na.strings = c("NA", ""))
testData <- read.csv("Testdata.csv", na.strings = c("NA", ""))
library(caret)
library(randomForest)
set.seed(938240)
```

Now, the training data is split into a 'new' training set and a set for validation. The decision is made to include 70% into the 'new' training set. 
```{r createValidation}
inTrain <- createDataPartition(y = trainingData$classe, p=0.70,list=F)
training <- trainingData[inTrain,]
validation <- trainingData[-inTrain,]
```

It's important to look at the data. It seems that there are quite a few variables with (many) missing values (For practical concerns the results of the overview are NOT shown here).
```{r overviewTraining, results = "hide"}
head(training)
```

With so many variables that have missing values, there are bound to be problems with the creation of the model. So, the decision is made to get rid of those variables of which more that 65% of the data is missing.
```{r ridofNA}
Keepvars <- c((colSums(!is.na(training[,-ncol(training)])) > 0.65*nrow(training)))
training <- training[,Keepvars]
validation <- validation[,Keepvars]
dim(training)
```

This leaves 60 variables. However, There are a few more variables, such as *X* and *user_name*, that have no added value. These need to be removed.
```{r rmvUseless}
uselessVars <- c("X","user_name","raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp",
                 "num_window", "new_window")
training <- training[,!(names(training) %in% uselessVars)]
validation <- validation[,!(names(validation) %in% uselessVars)]
dim(training)
```

Now, there are 53 variables left, including the the variable that our model is going to predict, *classe*.  

## Building the model

Now, the model is being build. The method of choice is **random forest**. This method has several benefits. First of all, it is used quite a lot and is a accurate method for prediction. Furthermore, it takes care of cross validation, so ther is no need to create extra subsamples of the training data. However, to provide a more certain overview of the out-of-sample error, the model will first, be run over the validation set.  
A negative aspect of random forest is that it might not be fastest method. Besides speed, there is also the issue that the model is a bit less interpretable.  
This part of the code builds the model in the training set.
```{r modelBuild}
modFit <- randomForest(classe ~., data = training)
modFit
```

The estimate of the Out-of-Bag error that the procedure calculates internally is 0.59%.

## Testing the model

The next step is to run the model on the validation set. This ensures that the estimation of the out-of-sample-error is more in line with the actual error.

```{r modelValidation}
predVal <- predict(modFit, newdata = validation)
confusionMatrix(predVal, validation$classe)
```

On the validation set, the accuracy of the model is 0.9947, suggesting that the out-of-sample error is around the 0.53%.

## Conclusion

This random forest model used 52 variables to predict the *classe*. The accuracy of the model on the validation set was 99.47% with an out-of-sample error of 0.53%.